{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis by hand and with NLTK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wer kennt sich mit jupyter notebooks aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install sklearn\n",
    "!{sys.executable} -m pip install xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#gives us the list of punctuations\n",
    "import string\n",
    "\n",
    "# for stopword removal. stopwords are like \"the\", \"and\", \"over\"\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# gives us functionality to stem a world. e.g. running -> run\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# gives us functionality of a blackbox sentiment analysis analyser (classes: positive, negative, neutral)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon') #pre-trained lexicon similar to AFINN\n",
    "\n",
    "# punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# for synonym translation\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import nltk.data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# calculating metrics of AFINN\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# in order to be able to get the precision/recall explanation from wikipedia\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset with user feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filepath to dataset\n",
    "fpDataset = './data/customer-feedback_full_cleaned_1000.xlsx'\n",
    "\n",
    "#Load Excel file into a DataFrame\n",
    "dfData = pd.read_excel(fpDataset, sheet_name='Sheet1')\n",
    "dfData_backup = dfData.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first feedback\n",
    "dfData['FEEDBACK'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter for only positive ratings\n",
    "dfData[dfData['RATING'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the AFINN-111 Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the AFINN mapping\n",
    "lol = list(csv.reader(open('data/AFINN-111.txt', 'r'), delimiter='\\t')) #load afinn into list of lists\n",
    "afinn = {d[0]: int(d[1]) for d in lol} #create afinn dictionary\n",
    "\n",
    "def afinnScore(word):\n",
    "    return afinn[word.lower()] if word.lower() in afinn else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the sentiment score for the Feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the afinn scores for one example\n",
    "sampleSentence = dfData['FEEDBACK'].loc[990]\n",
    "\n",
    "wordList = sampleSentence.split(' ')\n",
    "wordList_scores = [afinnScore(word) for word in wordList]\n",
    "\n",
    "print(sampleSentence)\n",
    "print(wordList_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#which words got scored?\n",
    "\n",
    "#get all scores in a dictionary\n",
    "scoredWords = dict(zip(wordList,wordList_scores))\n",
    "#get only the ones with value != 0\n",
    "scoredWords = {key: val for key, val in scoredWords.items() if val != 0}\n",
    "print(scoredWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The word OUTSTANDING is also in AFINN. but it has some punctuation on it. let's remove the punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removePunctuation(sentence):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in sentence if ch not in exclude)\n",
    "\n",
    "print(removePunctuation(sampleSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets apply to the whole dataset\n",
    "dfData['FEEDBACK'] = dfData['FEEDBACK'].apply(removePunctuation)\n",
    "sampleSentence = dfData['FEEDBACK'].loc[990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's put the afinn score for a sentence into a function and then apply to our sample sentence again \n",
    "\n",
    "def getAfinnScores(sentence): \n",
    "    wordList = sentence.split(' ')\n",
    "    wordList_scores = [afinnScore(word) for word in wordList] #repeating words are respected\n",
    "    sentenceScore = sum(wordList_scores)\n",
    "    \n",
    "    scoredWords = dict(zip(wordList,wordList_scores))\n",
    "    scoredWords = {key: val for key, val in scoredWords.items() if val != 0} #only get the scored words that matter\n",
    "    return sentenceScore,scoredWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getAfinnScores(sampleSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's apply this function to the whole dataset and add the information to the frame\n",
    "dfData['AFINN-score'] = dfData['FEEDBACK'].apply(getAfinnScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Stopwords using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we remove stopwords that are not required in the dataset, just to clean it up\n",
    "def removeStopWords(sentence):\n",
    "    stopwordList = stopwords.words(\"english\")\n",
    "    wordList = [word for word in sentence.split(' ') if removePunctuation(word.lower()) not in stopwordList]\n",
    "    return ' '.join(wordList)\n",
    "\n",
    "#see example:\n",
    "print(removePunctuation(sampleSentence), end='\\n-----------\\n')\n",
    "print(removeStopWords(removePunctuation(sampleSentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply it to the whole dataset\n",
    "dfData['FEEDBACK'] = dfData['FEEDBACK'].apply(removeStopWords)\n",
    "sampleSentence = dfData['FEEDBACK'].loc[990]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing with Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we define a function that takes a sentence and replaces each word by a canonical synonym\n",
    "def replace_synonyms(sentence):\n",
    "    output = \"\"\n",
    "    # Load the pretrained neural net\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    # Identify the parts of speech\n",
    "    tagged = nltk.pos_tag(words)\n",
    "\n",
    "    for i in range(0,len(words)):\n",
    "        synonyms = []\n",
    "        # Only replace nouns with nouns, vowels with vowels etc.\n",
    "        for syn in wn.synsets(words[i]):\n",
    "            # Do not attempt to replace proper nouns or determiners\n",
    "            if tagged[i][1] == 'NNP' or tagged[i][1] == 'DT':\n",
    "                break\n",
    "\n",
    "            # The tokenizer returns strings like NNP, VBP etc. but the wordnet synonyms has tags like .n.\n",
    "            # So we extract the first character from NNP ie n then we check if the dictionary word has a .n. or not \n",
    "            word_type = tagged[i][1][0].lower()\n",
    "            if syn.name().find(\".\"+word_type+\".\"):\n",
    "                r = syn.name()[0:syn.name().find(\".\")] # extract the word only\n",
    "                synonyms.append(r)\n",
    "\n",
    "        if len(synonyms) > 0:\n",
    "            output = output + \" \" + synonyms[0]\n",
    "        else:\n",
    "            # If no replacement could be found, then just use the original word\n",
    "            output = output + \" \" + words[i]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lets see the difference\n",
    "print(sampleSentence)\n",
    "print(replace_synonyms(sampleSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply to whole dataset\n",
    "dfData['FEEDBACK'] = dfData['FEEDBACK'].apply(replace_synonyms)\n",
    "sampleSentence = dfData['FEEDBACK'].loc[990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's update the afinn score based on cleaned feedbacks\n",
    "dfData['AFINN-score'] = dfData['FEEDBACK'].apply(getAfinnScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the performance of an approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first we need to normalize AFINN scores to make them comparable to the RATING scores\n",
    "def normalize_afinn(affinScore):\n",
    "    if affinScore > 0: return 1\n",
    "    else: return 0\n",
    "\n",
    "# an AFINN score of 0 is neutral. this doesnt exist in the Ranking.\n",
    "# Thus here we will use the \"normalize_afinn\" function only on non-zero values (we transform Zero values to NULL/NAN)\n",
    "# later we will filter out these invalid values\n",
    "def normalize_afinn_scores(dfData):\n",
    "    dfData['AFINN-score-normalized'] =  np.nan\n",
    "    dfScore = pd.DataFrame(list(dfData['AFINN-score']),columns=['sentence_score', 'word_scores'])\n",
    "    dfData['AFINN-score-normalized'] = dfScore[dfScore['sentence_score'] != 0]\n",
    "    dfData['AFINN-score-normalized'] = dfData[dfData['AFINN-score-normalized'].notnull()].loc[:,'AFINN-score-normalized'].apply(normalize_afinn)\n",
    "    return dfData\n",
    "    \n",
    "dfData = normalize_afinn_scores(dfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url='https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg')\n",
    "# Precision = TP / allSelected = TP / (TP+FP) \n",
    "    #-> \"Of all items identified as positive, how much % are correctly identified as positive?\" \n",
    "    # high precision means that an algorithm returned substantially more relevant results than irrelevant ones\n",
    "    \n",
    "# Recall = TP / allRelevant = TP / (TP+FN) \n",
    "    #-> \"of all positive items, how much %  are correctly identified as positive?\"\n",
    "    # high recall means that an algorithm returned most of the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_afinn_performance():\n",
    "    # lets filter out all NULL/NAN values (where the AFINN score was neutral at Zero)\n",
    "    dfScoring = dfData[dfData['AFINN-score-normalized'].notnull()]\n",
    "\n",
    "    # we can calculate the values manually:\n",
    "    true_positive =  len(dfScoring[(dfScoring['AFINN-score-normalized'] == 1) & (dfScoring['RATING'] == 1)]) # Algo identified as 1, and groundtruth is 1\n",
    "    false_positive = len(dfScoring[(dfScoring['AFINN-score-normalized'] == 1) & (dfScoring['RATING'] == 0)]) # Algo identified as 1, but it is 0\n",
    "    false_negative = len(dfScoring[(dfScoring['AFINN-score-normalized'] == 0) & (dfScoring['RATING'] == 1)]) # Algo didn't identify as 1, but it is 1\n",
    "\n",
    "    precision = true_positive / (true_positive+false_positive)\n",
    "    recall = true_positive / (true_positive+false_negative)\n",
    "    f1_score = 2 * precision*recall / (precision+recall) # Harmonic average of the precision and recall. Range [0,1]\n",
    "\n",
    "    return precision, recall, f1_score, dfScoring\n",
    "\n",
    "\n",
    "precision, recall, f1_score, dfScoring = get_afinn_performance()\n",
    "\n",
    "print('{} precision'.format(precision))\n",
    "print('{} recall'.format(recall))\n",
    "print('{} f1 score'.format(f1_score)) # Harmonic average of the precision and recall. Range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ....or use libraries for this purpose\n",
    "y_test = dfScoring['RATING']\n",
    "y_pred = dfScoring['AFINN-score-normalized']\n",
    "print('{} precision'.format(precision_score(y_test, y_pred)))\n",
    "print('{} recall'.format(recall_score(y_test, y_pred)))\n",
    "#print('{} f1 score'.format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now lets compare without having cleaned the data\n",
    "dfData['AFINN-score'] = dfData_backup['FEEDBACK'].apply(getAfinnScores)\n",
    "#update the normalized values\n",
    "dfData = normalize_afinn_scores(dfData)\n",
    "\n",
    "precision, recall, f1_score, dfScoring = get_afinn_performance()\n",
    "\n",
    "print('{} precision'.format(precision))\n",
    "print('{} recall'.format(recall))\n",
    "print('{} f1 score'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word stemming using NLTK's PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word stemming means trimming the word to its lexicographical stem.\n",
    "# this way multiple derivatives of words can be categorized as one. e.g. running -> run\n",
    "\n",
    "# In general it is good practice to stem all the words,\n",
    "# as some algorithms assume that the words have been stemmed with a specific algorithm\n",
    "\n",
    "def stemWords(sentence):\n",
    "    wordList = sentence.split(' ')\n",
    "    ps = PorterStemmer()\n",
    "    return ' '.join([ps.stem(word) for word in wordList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this stemmer is highly rule-based rather than lexicograph. It follows the principle of:\n",
    "# \"the purpose of stemming is to bring variant forms of a word together, not to map a word onto its ‘paradigm’ form.\"\n",
    "\n",
    "print(sampleSentence)\n",
    "print(stemWords(sampleSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's NOT apply to the whole dataset just yet\n",
    "\n",
    "#dfData['FEEDBACK'] = dfData['FEEDBACK'].apply(stemWords)\n",
    "#sampleSentence = dfData['FEEDBACK'].loc[990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one word of caution: there are several algorithms that assume different initial conditions.\n",
    "# an example is here: the goal of stemming was to make words more generalizeable. but the AFINN mapping assumes no stemming\n",
    "\n",
    "# for example for words in the \"family\" of \"affect\", AFINN has scores for the following:\n",
    "afinn_affect = [word for word in afinn if (word.startswith('affect'))]\n",
    "print([getAfinnScores(word) for word in afinn_affect], end =\"\\n----------\\n\")\n",
    "\n",
    "# the stemmed versions are as follows:\n",
    "ps_affect = [PorterStemmer().stem(word) for word in afinn_affect]\n",
    "print(ps_affect, end =\"\\n----------\\n\")\n",
    "\n",
    "#and these are the scores after the stemming\n",
    "print([getAfinnScores(word) for word in ps_affect], end =\"\\n----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the afinn score after stemming is negatively affected\n",
    "print(getAfinnScores(stemWords(sampleSentence)), end =\"\\n-------\\n\")\n",
    "\n",
    "# lThis is because the stemmer changed the word OUTSTAND to something that is not mapped in AFINN\n",
    "print([PorterStemmer().stem(word) for word in sampleSentence.split(' ')]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting scores from pre-trained black box model with NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# prints the scores for some feedbacks\n",
    "for sentence in dfData['FEEDBACK'].loc[4:5]:\n",
    "    print(sid.polarity_scores(sentence))\n",
    "    print(sentence, end='\\n------------\\n')\n",
    "    \n",
    "\n",
    "# here the vader model is already a trained model. \n",
    "# we use the model to calculate the sentiment score for the sentences in our dataset\n",
    "\n",
    "#source: https://opensourceforu.com/2016/12/analysing-sentiments-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as we do not know on which basis this blackbox algorithm works, \n",
    "# one has to test whether stemming, removing punctuations etc gives a better or worse result\n",
    "\n",
    "#here we use the unmodified dfData_backup\n",
    "for sentence in dfData_backup['FEEDBACK'].loc[4:5]:\n",
    "    print(sid.polarity_scores(sentence))\n",
    "    print(sentence, end='\\n------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training own generic classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now we use the labels from the dataset\n",
    "\n",
    "# here we do not have a pre-trained model. we use our own model to train a generic classifier\n",
    "    \n",
    "# Step 1 – Training data\n",
    "#labels = ['neg','pos','neg','pos','pos']\n",
    "dataset = list(zip(dfData[\"FEEDBACK\"],dfData[\"RATING\"]))\n",
    "  \n",
    "# Step 2\n",
    "dictionary = set(word.lower() for passage in dataset for word in word_tokenize(passage[0]))\n",
    "  \n",
    "# Step 3\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) for x in dataset]\n",
    "  \n",
    "# Step 4 – the classifier is trained with sample data\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = sampleSentence\n",
    "print(sampleSentence)\n",
    "\n",
    "test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n",
    "\n",
    "distribution = classifier.prob_classify(test_data_features)\n",
    "for label in distribution.samples():\n",
    "    print(\"%s: %f\" % (label, distribution.prob(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
