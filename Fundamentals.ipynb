{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis by hand and with NLTK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#gives us the list of punctuations\n",
    "import string\n",
    "\n",
    "# for stopword removal. stopwords are like \"the\", \"and\", \"over\"\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# gives us functionality to stem a world. e.g. running -> run\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# gives us functionality of a blackbox sentiment analysis analyser (classes: positive, negative, neutral)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon') #pre-trained lexicon similar to AFINN\n",
    "\n",
    "# \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# calculating metrics of AFINN\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# in order to be able to get the precision/recall explanation from wikipedia\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset with user feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filepath to dataset\n",
    "fpDataset = './data/customer-feedback_full_cleaned_1000.xlsx'\n",
    "\n",
    "#Load Excel file into a DataFrame\n",
    "dfExcelWorkbook = pd.read_excel(fpDataset, sheet_name=None)\n",
    "sheets = list(dfExcelWorkbook.keys())\n",
    "dfData = dfExcelWorkbook[sheets[0]]\n",
    "dfData_backup = dfData.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first feedback\n",
    "dfData['FEEDBACK'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter for only positive ratings\n",
    "dfData[dfData['RATING'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the AFINN-111 Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the AFINN mapping\n",
    "lol = list(csv.reader(open('data/AFINN-111.txt', 'r'), delimiter='\\t')) #load afinn into list of lists\n",
    "afinn = {d[0]: int(d[1]) for d in lol} #create afinn dictionary\n",
    "\n",
    "def afinnScore(word):\n",
    "    return afinn[word.lower()] if word.lower() in afinn else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the sentiment score for the Feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the afinn scores for one example\n",
    "sampleSentence = dfData['FEEDBACK'].loc[990]\n",
    "\n",
    "wordList = sampleSentence.split(' ')\n",
    "wordList_scores = [afinnScore(word) for word in wordList]\n",
    "\n",
    "print(sampleSentence)\n",
    "print(wordList_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#which words got scored?\n",
    "\n",
    "#get all scores in a dictionary\n",
    "scoredWords = dict(zip(wordList,wordList_scores))\n",
    "#get only the ones with value != 0\n",
    "scoredWords = {key: val for key, val in scoredWords.items() if val != 0}\n",
    "print(scoredWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The word OUTSTANDING is also in AFINN. but it has some punctuation on it. let's remove the punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removePunctuation(sentence):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in sentence if ch not in exclude)\n",
    "\n",
    "print(sampleSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets apply to the whole dataset\n",
    "dfData['FEEDBACK'] = dfData['FEEDBACK'].apply(removePunctuation)\n",
    "sampleSentence = dfData['FEEDBACK'].loc[990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's put the afinn score for a sentence into a function and then apply to our sample sentence again \n",
    "\n",
    "def getAfinnScores(sentence): \n",
    "    wordList = sentence.split(' ')\n",
    "    wordList_scores = [afinnScore(word) for word in wordList] #repeating words are respected\n",
    "    sentenceScore = sum(wordList_scores)\n",
    "    \n",
    "    scoredWords = dict(zip(wordList,wordList_scores))\n",
    "    scoredWords = {key: val for key, val in scoredWords.items() if val != 0} #only get the scored words that matter\n",
    "    return sentenceScore,scoredWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getAfinnScores(sampleSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's apply this function to the whole dataset and add the information to the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData['AFINN-score'] = dfData['FEEDBACK'].apply(getAfinnScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Stopwords using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we remove stopwords that are not required in the dataset, just to clean it up\n",
    "def removeStopWords(sentence):\n",
    "    stopwordList = stopwords.words(\"english\")\n",
    "    wordList = [word for word in sentence.split(' ') if removePunctuation(word.lower()) not in stopwordList]\n",
    "    return ' '.join(wordList)\n",
    "\n",
    "#see example:\n",
    "print(removePunctuation(sampleSentence), end='\\n-----------\\n')\n",
    "print(removeStopWords(removePunctuation(sampleSentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply it to the whole dataset\n",
    "dfData['FEEDBACK'] = dfData['FEEDBACK'].apply(removeStopWords)\n",
    "sampleSentence = dfData['FEEDBACK'].loc[990]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the performance of an approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url='https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_afinn(affinScore):\n",
    "    if affinScore > 0: return 1\n",
    "    else: return 0\n",
    "    \n",
    "\n",
    "dfScore = pd.DataFrame(list(dfData['AFINN-score']),columns=['sentence_score', 'word_scores'])\n",
    "dfData['AFINN-score-normalized'] = dfScore['sentence_score'].apply(normalize_afinn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = dfData['RATING']\n",
    "y_pred = dfData['AFINN-score-normalized']\n",
    "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word stemming using NLTK's PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Word stemming means trimming the word to its lexicographical stem.\n",
    "# this way multiple derivatives of words can be categorized as one. e.g. running -> run\n",
    "\n",
    "# In general it is good practice to stem all the words,\n",
    "# as some algorithms assume that the words have been stemmed with a specific algorithm\n",
    "\n",
    "def stemWords(sentence):\n",
    "    wordList = sentence.split(' ')\n",
    "    ps = PorterStemmer()\n",
    "    return ' '.join([ps.stem(word) for word in wordList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this stemmer is highly rule-based rather than lexicograph. It follows the principle of:\n",
    "# \"the purpose of stemming is to bring variant forms of a word together, not to map a word onto its ‘paradigm’ form.\"\n",
    "\n",
    "print(sampleSentence)\n",
    "print(stemWords(sampleSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's NOT apply to the whole dataset just yet\n",
    "\n",
    "#dfData['FEEDBACK'] = dfData['FEEDBACK'].apply(stemWords)\n",
    "#sampleSentence = dfData['FEEDBACK'].loc[990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one word of caution: there are several algorithms that assume different initial conditions.\n",
    "# an example is here: the goal of stemming was to make words more generalizeable. but the AFINN mapping assumes no stemming\n",
    "\n",
    "# for example for words in the \"family\" of \"affect\", AFINN has scores for the following:\n",
    "afinn_affect = [word for word in afinn if (word.startswith('affect'))]\n",
    "print([getAfinnScores(word) for word in afinn_affect], end =\"\\n----------\\n\")\n",
    "\n",
    "# the stemmed versions are as follows:\n",
    "ps_affect = [PorterStemmer().stem(word) for word in afinn_affect]\n",
    "print(ps_affect, end =\"\\n----------\\n\")\n",
    "\n",
    "#and these are the scores after the stemming\n",
    "print([getAfinnScores(word) for word in ps_affect], end =\"\\n----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the afinn score after stemming is negatively affected\n",
    "print(getAfinnScores(stemWords(sampleSentence)), end =\"\\n-------\\n\")\n",
    "\n",
    "# lThis is because the stemmer changed the word OUTSTAND to something that is not mapped in AFINN\n",
    "print([PorterStemmer().stem(word) for word in sampleSentence.split(' ')]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting scores from pre-trained black box model with NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# prints the scores for some feedbacks\n",
    "for sentence in dfData['FEEDBACK'].loc[4:5]:\n",
    "    print(sid.polarity_scores(sentence))\n",
    "    print(sentence, end='\\n------------\\n')\n",
    "    \n",
    "\n",
    "# here the vader model is already a trained model. \n",
    "# we use the model to calculate the sentiment score for the sentences in our dataset\n",
    "\n",
    "#source: https://opensourceforu.com/2016/12/analysing-sentiments-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# as we do not know on which basis this blackbox algorithm works, \n",
    "# one has to test whether stemming, removing punctuations etc gives a better or worse result\n",
    "\n",
    "#here we use the unmodified dfData_backup\n",
    "for sentence in dfData_backup['FEEDBACK'].loc[4:5]:\n",
    "    print(sid.polarity_scores(sentence))\n",
    "    print(sentence, end='\\n------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training own generic classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we use the labels from the dataset\n",
    "\n",
    "# here we do not have a pre-trained model. we use our own model to train a generic classifier\n",
    "    \n",
    "# Step 1 – Training data\n",
    "#labels = ['neg','pos','neg','pos','pos']\n",
    "dataset = list(zip(dfData[\"FEEDBACK\"],dfData[\"RATING\"]))\n",
    "  \n",
    "# Step 2\n",
    "dictionary = set(word.lower() for passage in dataset for word in word_tokenize(passage[0]))\n",
    "  \n",
    "# Step 3\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) for x in dataset]\n",
    "  \n",
    "# Step 4 – the classifier is trained with sample data\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = sampleSentence\n",
    "print(sampleSentence)\n",
    "\n",
    "test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n",
    "\n",
    "distribution = classifier.prob_classify(test_data_features)\n",
    "for label in distribution.samples():\n",
    "    print(\"%s: %f\" % (label, distribution.prob(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
